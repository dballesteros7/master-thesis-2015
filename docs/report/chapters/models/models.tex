\chapter{Models}
\label{sec:models}

This chapter explores different probabilistic models for the problem, explains how they can be efficiently learned from data and presents the learning algorithm performance on synthetic data.

\section{Submodular model: FLID}

The Facility Location Diversity (FLID) model was first proposed in \cite{tschiatschek16learning} and belongs to the class of log-submodular probability distributions over sets.

\begin{definition}
  A distribution over sets $S \subseteq V$, where w.l.o.g $V = \{1,\dots,|V|\}$, of the form $P(S) = \frac{1}{Z}\mathrm{exp}(F(S))$ is called a log-submodular probability distribution, if $F(S)$ is a submodular function \cite{djolonga14variational}.
\end{definition}

\begin{definition}
  \label{def:submodularity}
  A function $F:2^V \rightarrow \mathbb{R}$ is submodular if it exhibits a "diminishing returns" property \cite{krause14submodular}, namely:
    \begin{equation*}
      \forall S,T \subseteq V : S \subseteq T, i \notin T \mid F(S \cup i) - F(S) \geq F(T \cup i) - F(T)
    \end{equation*}
\end{definition}

Submodular functions intuitively indicate that adding an item to a smaller set results in a larger gain than adding it to a larger one. This is a natural property in the context of summarization where adding more information to a large summary is less effective than adding it to a smaller one.

The FLID model consists of two terms, first a modular one which considers the utility or relevance of the items in the set, namely:

\begin{equation}
  P(S) \propto \exp\left(\sum_{i \in S}u_{i}\right)
\end{equation}

Where $u_{i} \in \mathbb{R}$ quantifies the utility of item $i$. In the context of location summarization, this utility could be proportional to the popularity of a place and how many times it has been photographed.

The diversity term is based on the idea of a latent concept space of dimension $L$ where each item can be represented with a vector $\mathbf{w}^{D}_{i} \in \mathbb{R}^{L}_{\geq 0}$. The representation in this space allows the model to identify items that are similar and penalize sets that include them together. Formally, the diversity term for a set $S \subseteq V$ is:

\begin{equation}
  \label{eq:diversity}
  \mathrm{div}(S) = \sum_{d=1}^{L}\left(\max_{i \in S}{w^{D}_{i, d}} - \sum_{i \in S}{w^{D}_{i,d}}\right)
\end{equation}

Putting this two terms together results in the \ref{eq:flid} probability model proposed in \cite{tschiatschek16learning}.

\begin{equation}
  \tag{FLID}
  P(S) = \frac{1}{Z}\exp{\left(\sum_{i \in S}u_{i} + \sum_{d=1}^{L}\left(\max_{i \in S}{w^{D}_{i, d}} - \sum_{i \in S}{w^{D}_{i,d}}\right)\right)}
  \label{eq:flid}
\end{equation}

In this model, $\mathbf{u} \in \mathbb{R}^{|V|}$ is the vector of utilities and $\mathbf{W}^{D} \in \mathbb{R}_{\geq 0}^{|V| \times L}$ is the diversity weight matrix where each row is the aforementioned $\mathbf{w}^{D}_{i}$ vector.

\subsection{Partition function}

In log-submodular probabilistic models, the normalization constant $Z$ is known as the \textit{partition function} \cite{djolonga14variational} and its exact computation is known to be \#P-complete \cite{jerrum1990}.  However, it has been proven \cite{tschiatschek16learning} that for FLID the partition function can be computed exactly in $\mathcal{O}(|V|^{L+1})$ time, which can be efficient for $L \ll |V|$. This is an important property because the partition function is necessary to compute marginal probabilities and other quantities.

\subsection{Example: Two landmarks}
\label{sec:flid-toy}

In order to illustrate the model, consider a town with 3 popular locations: A town hall ($h$), a statue ($s$) and a fountain ($f$). Data shows that visitors only take photos at the town hall and the statue, or at the town hall and the fountain. This can be modeled with FLID, introducing a latent concept that discourages taking photos of both the fountain and statue.

Concretely, let $V = \{h, s, f\}$ and $\mathbf{u} = \left(2, 2, 2\right)$, indicating that all locations are equally popular. A suitable diversity weight vector would then be $\mathbf{W}^{D} = \left(0, 20, 20\right)^{\intercal}$. Table \ref{tab:flid-toy-probs} shows the resulting probabilities of the subsets, accurately representing the aforementioned description of the problem.

\begin{table}
  \centering
  \caption{FLID probability distribution for the scenario in example \ref{sec:flid-toy}}
  \begin{tabular}{@{}ll@{}}
    \toprule
      $S$ & $P(S)$  \\
    \midrule
      $\{h,s\}, \{h,f\}$ & $\approx 0.41$ \\
      $\{h\}, \{s\}, \{f\}$ & $\approx 0.06$ \\
      $\{\}, \{s,f\}, \{h,s,f\}$ & $\approx 0.00$ \\
    \bottomrule
  \end{tabular}
  \label{tab:flid-toy-probs}
\end{table}

\section{Mixed model: FLDC}

Diversity is an important property in the context of summarization \cite{tschiatschek16learning}, however coherence is also a desired property of summaries, especially in the context of structured timeline summarization \cite{Yan:2011:ETS:2009916.2010016}. Balancing coherence and diversity is considered a challenge because maxiziming only one of these properties may lead to poor results on the other one \cite{Shahaf2012}.

In order to model coherence in this model, the addition of a log-supermodular term analogous to the diversity term \ref{eq:diversity} is proposed.

\begin{definition}
  \label{def:supermodularity}
  A function $F:2^V \rightarrow \mathbb{R}$ is supermodular iff $-F(S)$ is submodular.
\end{definition}

The supermodular term encodes the items into another latent concept space, of dimension $K$, where sets containing items with high values in some latent dimension are rewarded. Hence modeling complementarity between items.

Concretely, consider a spatial summary of a city where people tend to stay close to the city center. A possible latent dimension could encode the distance to the center, and rewarding coherence on this dimension would create summarizes where all locations are close together which is the modeled behavior.

The extended model will be reffered to as the Facility Location Diversity and Coherence (\ref{eq:fldc}) model and its probability distribution is:

\begin{equation}
  \tag{FLDC}
  P(S) = \frac{1}{Z}\exp{\left(\sum_{i \in S}{u_{i}} + \mathrm{div}(S) + \sum_{c=1}^{K}{\left(\sum_{i \in S}{w^{C}_{i,c}} - \max_{i \in S}{w^{C}_{i,c}}\right)}\right)}
  \label{eq:fldc}
\end{equation}

Where $\mathbf{w}_{i}^{C}$ is the $i$-th row of the $\mathbf{W}^{C} \in \mathbb{R}_{\geq 0}^{|V| \times K}$ matrix and corresponds to the representation of item $i$ in the concept space of dimension $K$.

It should be noted that the FLDC model is neither log-submodular or log-supermodular unless $K=0$ or $L=0$, respectively. However, it can be used and learned in a similar fashion as the FLID model.

\subsection{Example: Disjoint pairs}
\label{sec:fldc-toy}

As an example of the extended model, consider the distribution presented in table \ref{tab:fldc-toy-probs} for $V = \{1,2,3,4\}$. It represents a set of two disjoint pairs, which indicates there exists a diversity component between the two pairs whilist having a coherence component between the items contained in each pair.

Concretely, the weight matrices $\mathbf{W}^{D}, \mathbf{W}^{C}$ in figure \ref{fig:fldc-toy-mixed-weights} illustrate one possible instance of the model. The corresponding utility vector is $\mathbf{u} = \overrightarrow{0}$, because there is no indication that individual items are favored over the pairs. Note that this model is easily interpretable and accurately realizes the distribution.

\begin{table}
  \centering
  \caption{Probability distribution for example \ref{sec:fldc-toy}}
  \begin{tabular}{@{}ll@{}}
    \toprule
    $S$ & $P(S)$  \\
    \midrule
    $\{1,2\}, \{3,4\}$ & $0.5$ \\
    $2^V \setminus \{\{1,2\}, \{3,4\}\}$ & $0.0$ \\
    \bottomrule
  \end{tabular}
  \label{tab:fldc-toy-probs}
\end{table}

\begin{figure}
  \centering
  \includegraphics[width=\textwidth]{fldc_toy_example_mixed_weights}
  \caption{Diversity (left) and coherence weights (right) for FLDC model in example \ref{sec:fldc-toy}. The dotted line divides the paired items.}
  \label{fig:fldc-toy-mixed-weights}
\end{figure}


\section{Featurized model: FFLDC}

An important characteristic of the FLDC and FLID models is that they are agnostic to the type of items in the ground set, this allows its application to a wide range of problems without prior knowledge. However the downside is that the model has no capability to make use of information about the items, if available, to improve the modeling of the data. Moreover, if a new item is added to the set there is no way to generalize the existing knowledge about similar items to it.

In order to solve these problems, a further extension to the model is proposed. Firstly, the information about each item ($i \in V$) is represented as a vector $\mathbf{x}_{i} \in \mathbb{R}^{M}$ where each component is a feature, e.g. for venues one feature could be its aggregated rating while another indicates whether it is indoors or outdoors.

Then, the utility vector $\mathbf{u}$ and weight matrices $\mathbf{W}^{D}, \mathbf{W}^{C}$ are factorized to include the feature matrix $X \in \mathbb{R}^{|V| \times M}$ as follows:

\begin{align}
  \mathbf{u} &= \mathbf{Xa}   \label{eq:ffldc-factorization-1} \\
  \mathbf{W}^{D} &= \mathbf{XD}  \label{eq:ffldc-factorization-2} \\
  \mathbf{W}^{C} &= \mathbf{XC}
  \label{eq:ffldc-factorization-3}
\end{align} 

Where $a \in \mathbb{R}^{M}$ represents the contribution of each feature to the total utility of an item, whilist $\mathbf{D} \in \mathbb{R}^{M \times L}$ and $\mathbf{C} \in \mathbb{R}^{M \times K}$ encode the contribution of each feature to each latent diversity and coherence dimension, respectively. The intuition behind this factorization is that the information about the items can enhance the latent representations, hence producing a richer model.

The extended model will be reffered to as the Featurized Facility Location Diversity and Coherence (FFLDC) model and its probability distribution is:

\begin{align}
  \tag{FFLDC}
  P(S) &= \frac{1}{Z}\exp{\left(\sum_{i \in S}{\mathbf{X}_{i,*}\mathbf{a}} + fdiv(S) + fcoh(S)\right)} \\
  fdiv(S) &= \sum_{d=1}^{L}{\left(\max_{i \in S}{\mathbf{X}_{i,*}\mathbf{D}_{*,d}} - \sum_{i \in S}{\mathbf{X}_{i,*}\mathbf{D}_{*,d}}\right)} \\
  fcoh(S) &= \sum_{c=1}^{K}{\left(\sum_{i \in S}{\mathbf{X}_{i,*}\mathbf{C}_{*,c}} - \max_{i \in S}{\mathbf{X}_{i,*}\mathbf{C}_{*,c}}\right)}
  \label{eq:ffldc}
\end{align}

\begin{remark}
  If $X = \mathcal{I}$, then FFLDC is equivalent to FLDC.
\end{remark}

The use of features also allows the extension of the model to previously unknown items, hence solving the aforementioned problem of generalization. This is because the parameters of the FFLDC model, i.e. $\mathbf{a}, \mathbf{D}, \mathbf{C}$, do not depend on the ground set $V$ but rather on the space of features $\mathbb{R}^{M}$. If an item $j \notin V$ is considered, a model learned on only items in $V$ can immediately be applied to the new set $V \cup \{j\}$, contrary to the case of FLID or FLDC where it would require adding a new row to the weight matrices and learning its components.

\subsection{Example: Rated locations}
\label{sec:ffldc-toy}

A simple town has 6 popular locations, each of them has been rated from 0 (terrible) to 5 (excellent). It is known that a typical visitor cares about these ratings but also is interested in visiting places that are outdoors and/or serve food. Given data from previous visitors, shown in table \ref{tab:ffldc-toy-probs}, the task is to model this behavior using the FFLDC model.

\begin{table}
  \centering
  \caption{Synthetic data for example \ref{sec:ffldc-toy}}
  \begin{tabular}{@{}ll@{}}
    \toprule
    Locations visited $(S)$ & $P(S)$  \\
    \midrule
    $\{0,2\}$ & $0.29$ \\
    $\{2,3\}$ & $0.26$ \\
    $\{2,5\}$ & $0.14$ \\
    $\{0\}, \{1\}$ & $0.05$ \\
    $\{2\}, \{3\}, \{4\}, \{5\}$ & $0.04$ \\
    \bottomrule
  \end{tabular}
  \label{tab:ffldc-toy-probs}
\end{table}

In this example, there is knowledge about the items and what features are relevant for the data. These features are summarized in the matrix $\mathbf{X}$ in equation \ref{eq:ffldc-toy-feats}. The first column corresponds to the aforementioned rating, the second and third are binary features indicating whether the location is an outdoor one and whether it serves food, respectively.

\begin{equation}
  \mathbf{X} = \left(
    \begin{array}{ccc}
      4 & 1 & 0 \\
      4 & 1 & 1 \\
      3 & 0 & 1 \\
      3 & 1 & 0 \\
      2 & 1 & 1 \\
      2 & 1 & 0  \\
     \end{array}
  \right)
  \label{eq:ffldc-toy-feats}
\end{equation}

Looking at the data and features, it is possible to draw a FFLDC model that encourages diversity on the second a third feature while assigning a positive utility and coherence values to the first feature. One such model is presented in figure \ref{fig:ffldc-toy-all-weights} and accurately realizes the distribution form table \ref{tab:ffldc-toy-probs}.

\begin{figure}
  \centering
  \includegraphics[width=\textwidth]{ffldc_toy_example}
  \caption{FFLDC sample model for example \ref{sec:ffldc-toy}.}
  \label{fig:ffldc-toy-all-weights}
\end{figure}

With this model, if a new location $j$ is considered then it is straightforward to estimate what its probability of being visited $P(j \in S)$ would be, only knowing its feature representation.

\section{Learning from data}

Having described the models, the next step is to devise a way to learn them efficiently from data. As noted in \cite{tschiatschek16learning}, maximum likelihood estimation (MLE) is intractable in FLID due to the complexity of calculating the partition function for large $L$. This result can be extended to the FLDC model using the fact that in the algorithm presented in \cite{tschiatschek16learning} the weights can be either positive or negative, therefore it can include the coherence weights as negative diversity weights.

\begin{proposition}
  The time complexity of calculating the partition function for the FLDC model is $\mathcal{O}(|V|^{L+M+1})$, using a modified version of the algorithm presented in \cite{tschiatschek16learning}.
\end{proposition}

Additionally, this result can be extended to the FFLDC case because an FFLDC model can always be converted to an equivalent FLDC model through the factorization in \ref{eq:ffldc-factorization-1}-\ref{eq:ffldc-factorization-3}. This has a time complexity of $O(M|V|(L+K))$, corresponding to the matrix multiplications, which is considerably smaller than the complexity of the partition function computation for FLDC.

As an alternative to MLE, Noise Constrastive Estimation (NCE) has been proposed as a method for estimating unnormalized probablistic models from observed data \cite{Gutmann12NCE}. The following section describes this method in more detail and its application to the models.

\subsection{NCE Learning}

The idea behind NCE, as presented in \cite{Gutmann12NCE}, is to transform the unsupervised learning task of estimating a probability density from data into a supervised classification task. In order to do this, the observed data $\mathcal{D}$, assumed to be drawn from an unknown distribution $P_{d}$, is compared to an artificially generated set of noise samples $\mathcal{N}$ drawn from a known distribution $P_{n}$. The classification task is setup to optimize the likelihood of correctly discriminating each sample as either data or noise.

Formally, denote $\mathcal{A}$ as the complete set of labeled samples, i.e. $\mathcal{A} = \{(S,Y_{s}) : S \in \mathcal{D} \cup \mathcal{N}\}$ where $Y_{s} = 1 \equiv S \in \mathcal{D}$ and $Y_{s} = 0 \equiv S \in \mathcal{N}$. Additionally, $\eta$ is the ratio between noise and data samples, i.e. $\eta = \nicefrac{|\mathcal{N}|}{|\mathcal{D}|}$.

The goal is to estimate the posterior probabilities $P(Y_{s} = 1 \mid S;\theta)$ and $P(Y_{s} = 0 \mid S;\theta)$, in order to discriminate noise from data samples. These are defined as:

\begin{align}
  P(Y_{s} &= 1 \mid S;\theta) = \frac{\hat{P}_{d}(S;\theta)}{\hat{P}_{d}(S;\theta) + \eta P_{n}(S)} \\
  P(Y_{s} &= 0 \mid S;\theta) = \frac{\eta P_{n}(S)}{\hat{P}_{d}(S;\theta) + \eta P_{n}(S)}
\end{align}

It is worth noting that estimated distribution $\hat{P}_{d}$ is used instead of $P_{d}$, because the real density is unknown. As indicated in \cite{Gutmann12NCE}, $\hat{P}_{d}$ can be an unnormalized distribution for NCE where the partition function $Z$ is included in the set of parameters $\theta$ as $\hat{Z}$.

Estimating the posterior probabilities is equivalent to maximizing the conditional log-likelihood objective in \ref{eq:log-likelihood} \cite{Gutmann12NCE}.

\begin{equation}
  \label{eq:log-likelihood}
  g(\theta) = \sum_{S \in \mathcal{D}}{\log{P(Y_{s} = 1 \mid S;\theta)}} + \sum_{S \in \mathcal{N}}{\log{P(Y_{s} = 0 \mid S;\theta)}}
\end{equation}

In the case of the models described before, the parameters to adjust in order to maximize this objective are:

\begin{itemize}
  \item FLID \cite{tschiatschek16learning}: $\theta = [\hat{Z}, \mathbf{u}, \mathbf{W}^D]$.
  \item FLDC: $\theta = [\hat{Z}, \mathbf{u}, \mathbf{W}^D, \mathbf{W}^{C}]$.
  \item FFLDC: $\theta = [\hat{Z}, \mathbf{a}, \mathbf{D}, \mathbf{C}]$.
\end{itemize}

Finally, a couple of important conditions for NCE to work \cite{Gutmann12NCE}, are:

\begin{enumerate}
  \item The parameterized probability function $\hat{P}_{d}(S;\theta)$ must be of the same family as the real distribution $P_{d}$, i.e. $\exists \theta^{*} \mid \hat{P}_{d}(S;\theta^{*}) = P_{d}$.
  \item The noise distribution $P_{n}$ is nonzero whenever $P_{d}$ is nonzero.
\end{enumerate}

There are no constraints in the optimization method to use for this problem \cite{Gutmann12NCE}. Stochastic Gradient Descent (SGD) will be used for the proposed models, this is the same method used in \cite{tschiatschek16learning} for the FLID model. SGD is a gradient-based method that has proven effective in large-scale learning tasks due to its efficiency when the computation time is a limiting factor \cite{Bottou2010}\cite{Zhang2004}, hence making it appropriate for the scale of the problem considered in this work.

In each iteration, the sub-gradient of the objective function $g(\theta)$, this reduces to the computation of $\nabla P(Y_{s} = y \mid S;\theta)$. The following sections present the sub-gradient for each of the proposed models.

\subsubsection{FLID}

Show the gradient referencing from the AISTATS paper.

\subsubsection{FLDC}

Add the gradient term for the coherence term.

\subsubsection{FFLDC}

Derive the gradients with features, show that the are the same as FLDC with X = I.

\subsection{Learning FLID}

Show the results from learning the synthetic dataset from section \ref{sec:flid-toy}.

\subsection{Learning FLDC}

Show the results from learning the synthetic dataset from section \ref{sec:fldc-toy}.

\subsection{Learning FFLDC}

Show the results from learning the synthetic dataset from section \ref{sec:ffldc-toy}.

\subsection{Adagrad}

Quickly summarize the method, mention that it will be useful later.

\subsection{Hyper-parameter sensitivity}

Use the synthetic datasets to show the effect of the following parameters:

\begin{itemize}
  \item Number of iterations
  \item Noise size
  \item Step size (with/without adagrad)
\end{itemize}

