\chapter{Synthetic Experiments}
\label{sec:synthetic}

This chapter describes experiments done on synthetic data to explore the learning performance of the methods described in the background chapter.

\section{Learning Setup}

This section describes implementation details of NCE and SGD, these are applicable for both the experiments in this chapter as well as those performed on real data, which will presented in subsequent chapters.

\subsection{Noise Distribution}

As mentioned in section \ref{sec:nce-practical}, there are several considerations when choosing the noise distribution. A log-modular distribution is a natural choice for the noise when learning the FLID model \citep{tschiatschek16learning}, and this distribution will also be used for the noise with FLDC and FFLDC models.

The parameters of a log-modular distribution can be estimated efficiently from data through MLE, as given by Equation \eqref{eq:modular-mle}. Also the partition function can be computed in closed form, as shown in Equation \eqref{eq:modular-z}. This makes the distribution a good candidate because it can be used efficiently for sampling as well as for computing the normalized probability of sets, i.e. $P_{n}(S)$ during the learning procedure.

\subsection{Initialization}

The log-modular model also provides a sensible initialization for vector $\mathbf{u}$ in the FLID and FLDC models. For FFLDC, the initialization for $\mathbf{a}$ is done by solving the linear system derived from the factorization in Equation \eqref{eq:ffldc-factorization-1}, this linear system is given by:

\begin{equation}
  \label{eq:modular_features}
  \mathbf{a} = \mathbf{X}^{-1}\mathbf{u}
\end{equation}

Unfortunately, this system can be under/over-determined in many cases so an approximated solution must be used. The chosen method for finding this solution is ridge regression which minimizes the squared error $\sum_{i \in S}(u_{i} - (\mathbf{X}\mathbf{a})_{i})^{2}$ with a regularization term on the $l_{2}$ norm of $\mathbf{a}$.

The weight matrices, e.g. $\mathbf{W}^{e}, \mathbf{B}$, are initialized with random values drawn from a uniform distribution $\mathcal{U} \sim [0, 0.001]$. This is the same initialization setting proposed by \citet{tschiatschek16learning}.

In the cases where AdaGrad is used, the accumulated gradient $\mathbf{G}$ is initialized with a constant value for each parameter $\gamma$, namely $G^{0}_{\gamma} = 0.01$.

\subsection{Learning Rate}

When not using AdaGrad, the learning rate used in the experiments follows the power law mentioned in section \ref{sec:adagrad}, i.e.

\begin{equation}
  \eta(t) = \frac{\eta_{0}}{t^{p}}
\end{equation}

\subsection{Projection in SGD}

The weight matrices, e.g. $\mathbf{W}^{e}, \mathbf{B}$, are defined only for positive values. Therefore, they must be projected in each gradient step to ensure that the solution stays in the feasible space. This is done following the procedure proposed by \citet{tschiatschek16learning} which produces better results than simply clipping the values to 0. Let $\theta_{h}$ be an element of these matrices, e.g. $\theta_{h} = w^{e}_{i,d}$, then the projection is defined as:

\begin{equation}
  \theta_{h} = \begin{cases}
    \theta'_{h} & \text{if}\ \theta'_{h} \geq 0 \\
    \mathcal{U} \sim [0, 0.001] & \text{otherwise}
  \end{cases}
\end{equation}

Where $\theta'_{h}$ is the unnormalized parameter after the gradient update and $\mathcal{U} \sim [0, 0.001]$ means that the value is drawn from an uniform distribution between $[0, 0.001]$ as in the initialization.

\subsection{Column Restarts}

One common problem observed during the experiments is resulting models with two or more columns in the weight matrices which are very similar and therefore redundant. This can happen for some initialization settings, e.g. when $i^{d}$ and $i^{c}$ in Equations \eqref{eq:sub-b} and \eqref{eq:sub-e} are the same initially and increase at the same rate, hence leading to a local maximum.

In order to address this problem, a heuristic was added to the implementation that resets one of the columns when it is too similar to another one. This similarity is measured as:

\begin{equation}
  \mathrm{sim}_{\mathbf{W}}(a,b) = \hat{\mathbf{W}}_{*,a}^{\intercal} \cdot \hat{\mathbf{W}}_{*,b}
\end{equation}

Where $a$ and $b$ are column indexes for a weight matrix $\mathbf{W}$, e.g. $\mathbf{W}^{b}$, and $\hat{\mathbf{W}}_{*,a}$ and $\hat{\mathbf{W}}_{*,b}$ are the normalized $a$-th and $b$-th columns of $\mathbf{W}$, respectively.

The column pair with maximum similarity is identified every $N_{r}$ iterations and if the similarity is larger than a threshold $\delta$ then the first column is added to the second one, and the first one is restarted to random values as done in the initialization step. The expectation is that the reset column will learn different information with the new initialization values. Usually $N_{r} = 500$ and $\delta = 0.9$ was used.

\subsection{Termination Criteria}

The termination criteria for learning was the number of epochs, i.e. full passes over the data and noise samples.

\section{Example Datasets}

This section presents the results obtained after learning the datasets from the examples given for each model. For these experiments, the settings were:

\begin{itemize}
  \item 1000 samples of data drawn from the corresponding distribution, i.e. $|\mathcal{D}| = 1000$.
  \item 2000 noise samples, i.e. $\nu = 2$.
  \item 100 epochs.
  \item $\eta_{0}$ is 0.005 and $p = 0.1$ for the learning rate, without AdaGrad.
\end{itemize}

It is worth noting that the presented results for these experiments correspond to the best of multiple runs. It was observed that the learning algorithm does not always arrive to a good solution. This is because $g(\boldsymbol{\theta})$ is not a concave function and some initializations lead to local maxima.

In order to compare the models, the Total Variation Distance (TVD) between the target distribution $P_{d}$ and the learned distribution $\hat{P}_{d}$ was used, intuitively this measures how good is the approximation of the learned model to the real distribution. This is defined as \citep[chap. 4]{levin2009markov}:

\begin{equation}
  \label{eq:total_var}
  \|P_{d} - \hat{P}_{d}\|_{TV}= \frac{1}{2}\sum_{S \subseteq V}|P_{d}(S) - \hat{P}_{d}(S)|
\end{equation}

\subsection{FLID: Two Landmarks}

Recall the following characteristics about the dataset in Example \ref{sec:flid-toy}:

\begin{itemize}
  \item There are 3 elements, $h$, $s$, $f$.
  \item There are only 2 possible sets, $\{h,s\}$ and $\{h,f\}$. Both equally likely, i.e. $P(S) = 0.5$.
  \item A model can be constructed with a single diversity dimension, i.e. $L=1$.
\end{itemize}

After estimating the parameters from this data, the resulting utility vector and weight matrix are:

\begin{align*}
  \mathbf{u} = \left(5.42,2.82,2.79\right)^{\intercal} \\
  \mathbf{W}^{b} = \left(0.02, 6.10, 6.10\right)^{\intercal}
\end{align*}

This is a slightly different model from the one suggested in section \ref{sec:flid-toy}, however the normalized probability distribution for this model presented in Table \ref{tab:flid-toy-learned-probs} shows that it closely approximates the distribution.

\begin{table}
  \centering
  \caption{Learned distribution for Example \ref{sec:ffldc-toy}}
  \begin{tabular}{@{}ll@{}}
    \toprule
    $S$ & $P(S)$\\
    \midrule
    $\{h,s\}$ & $0.48$ \\
    $\{h,f\}$ & $0.47$ \\
    $\{h\}$ & $0.03$ \\
    $\{h,s,f\}$ & $0.02$ \\
    \bottomrule
  \end{tabular}
  \label{tab:flid-toy-learned-probs}
\end{table}

\subsection{FLDC: Two Non-overlapping Clusters}

Recall the following characteristics about the dataset in Example \ref{sec:fldc-toy}:

\begin{itemize}
  \item There are 4 items, i.e. $V = \{1,2,3,4\}$.
  \item There are only 2 possible sets, $\{1,2\}$ and $\{3,4\}$. Both equally likely, i.e. $P(S) = 0.5$.
  \item A model can be constructed with two diversity and two coherence dimensions, i.e. $L=2,K=2$.
\end{itemize}

Figure \ref{fig:fldc-toy-learned-weights} shows the resulting utility vector $\mathbf{u}$ and weight matrices $\mathbf{W}^{b}, \mathbf{W}^{e}$ after the learning procedure.

\begin{figure}
  \centering
  \includegraphics[width=\textwidth]{fldc_toy_example_learned_weights}
  \caption{Learned model for Example \ref{sec:fldc-toy}. The white line divides the clusters described in the example.}
  \label{fig:fldc-toy-learned-weights}
\end{figure}

The model shown in the figure displays the desired properties of diversity between the clusters while having coherence between the items in each one. Despite being different from the proposed model in section \ref{sec:fldc-toy} it also approximately realizes the desired distribution with a total variation distance equal to $0.04$ which represents a very small error in approximating the distribution from Table \ref{tab:fldc-toy-probs}.

\subsubsection{FFLDC: Rated Locations}

Recall the following characteristics about the dataset in Example \ref{sec:ffldc-toy}:

\begin{itemize}
  \item There are 6 items, i.e. $V = \{1,2,3,4,5,6\}$.
  \item The full distribution is related to the features and is given in Table \ref{tab:ffldc-toy-probs}.
  \item The proposed model has 2 diversity dimensions and one coherence dimension.
\end{itemize}

\begin{figure}
  \centering
  \includegraphics[width=\textwidth]{ffldc_toy_example_learned_weights}
  \caption{Learned model for Example \ref{sec:ffldc-toy}}
  \label{fig:ffldc-toy-learned-weights}
\end{figure}

Figure \ref{fig:ffldc-toy-learned-weights} shows the resulting model after learning. The total variation distance for the model in Figure \ref{fig:ffldc-toy-learned-weights} from the distribution in Table \ref{tab:ffldc-toy-probs} is $0.08$, indicating a very small error in the approximation. Unfortunately, this model can be not as easily interpreted as the one proposed in figure \ref{fig:ffldc-toy-all-weights}, nevertheless it accurately models the distribution from the example thus showing the learning algorithm's ability to estimate a distribution using features. 

\section{Learning Rate Sensitivity}

This section explores the learning rate's effect on the learning performance for the synthetic dataset from Example \ref{sec:ffldc-toy}.

The first experiment considers the stability of the optimization without AdaGrad as the learning rate increases. The settings are the same as for the previous experiments with the following additions:

\begin{itemize}
  \item $\eta_{0}$ is increased between $0.0005$ and $0.02$
  \item The learning is performed independently 8 times to obtain statistics on the results.
  \item For these experiments the reset heuristic for columns was disabled to prevent sudden changes in the objective function due to the resets.
\end{itemize}

Figure \ref{fig:effects_eta_0} shows $g(\boldsymbol{\theta})$ during the optimization, each point corresponds to the mean value calculated after a given number of epochs and the error bars show the corresponding 95\% confidence interval for the mean.

\begin{figure}
  \centering
  \includegraphics[width=\textwidth]{effects_eta_0_ffldc}
  \caption{Objective function for various values of $\eta_0$ without using AdaGrad.}
  \label{fig:effects_eta_0}
\end{figure}

The results in Figure \ref{fig:effects_eta_0} show that for small values of $\eta_0$ the learning can be significantly slow and that large values of $\eta_0$ may lead to instability, as discussed in section \ref{sec:adagrad}. Concretely, for $\eta_{0} \leq 0.01$ the learning is slow but stable, while for $\eta_{0} \geq 0.01$ the objective value converges quickly but oscillates across epochs. This oscillation is problematic because it breaks the guarantee that the objective value is better as the number of epochs increase. Fortunately, in the particular case of this synthetic dataset there exists a learning rate for which the objective function converges and is stable, i.e. $\eta_{0} = 0.005$ however this may not be always the case for real data.

By contrast, Figure \ref{fig:effects_adagrad} shows the objective function for $\eta_{0}$ between $0.005$ and $0.1$ after incorporating AdaGrad to the implementation. These results show that training with AdaGrad is stable for a larger range of $\eta_0$ without sacrificing convergence speed.

\begin{figure}
  \centering
  \includegraphics[width=\textwidth]{effects_eta_0_ffldc_adagrad}
  \caption{Objective function during learning for various values of $\eta_0$ with AdaGrad.}
  \label{fig:effects_adagrad}
\end{figure}

Finally, figure \ref{fig:comparison_adagrad_ffldc_toy} shows a direct comparison between training with and without AdaGrad using the best values of $\eta_0$ for each configuration. In this figure both algorithms converge to an optimal value, however it also shows that training with AdaGrad converges significantly faster while mantaining a more stable mean. These combined results indicate that using AdaGrad is the best strategy for learning the FFLDC model.

\begin{figure}
  \centering
  \includegraphics[width=\textwidth]{ffldc_adagrad_comparison}
  \caption{Comparison of learning performance with and without AdaGrad.}
  \label{fig:comparison_adagrad_ffldc_toy}
\end{figure}

\section{Noise-to-data Ratio}

The noise-to-data ratio $\nu$ is another hyperparameter for NCE learning, the recommendation is to have as many noise samples as computationally possible \citep{Gutmann12NCE}. In order to better understand the effect of $\nu$, an experiment was performed where different models for the dataset from Example \ref{sec:ffldc-toy} are learned with different values of $\nu$.

The setting is the same as for the learning rate experiments, but with $\eta_{0} = 0.05$ and always using AdaGrad. The noise parameter $\nu$ was varied between $0.5$ and $10$.

The results for this experiment are shown in Figure \ref{fig:effects_noise_ffldc}. It shows that increasing the number of noise samples reduces the variation distance of the resulting model as expected. However, it only decreases signficantly for small values of $\nu$, this may be because the number of epochs is not sufficient for larger $\nu$. This last assumption was tested by running another experiment with fixed $\nu = 10$ but increasing the number of epochs from 100 to 1500, the results are presented in Figure \ref{fig:effects_epochs_ffld}.

\begin{figure}
  \centering
  \includegraphics[width=\textwidth]{effects_noise_ffldc}
  \caption{Total variation distance for different values of $\nu$.}
  \label{fig:effects_noise_ffldc}
\end{figure}

\begin{figure}
  \centering
  \includegraphics[width=\textwidth]{effects_epochs_ffldc}
  \caption{Total variation distance for different number of epochs.}
  \label{fig:effects_epochs_ffld}
\end{figure}

Figure \ref{fig:effects_epochs_ffld} show that increasing the number of epochs can significantly improve the quality of the learned model. The decreasing distance indicates that it is possible for the NCE learning procedure to converge to the true distribution given sufficient noise samples and epochs.