\chapter{Background}
\label{sec:background}

This chapter presents the supporting concepts and methods that will be used throughout this work.

\section{Submodular Probability Set Functions}

As described before, probability models over sets are of great interest and can be applied to diverse settings. In particular, submodular probability set functions have been utilized in multiple domains, such as document summarization and information gathering \citep{krause14submodular}.

Probability set functions are a class of distributions $P$ over subsets $S$ of a ground set $V$, i.e.  functions that satisfy $\forall S \subseteq V, P: 2^V \rightarrow \mathbb{R} \mid 0 \leq P(S) \leq 1$ and $\sum_{S \subseteq V} P(S) = 1$. These distributions can also be understood as probabilities over binary sequences $[a_{1}, \dots,a_{n}]$ of fixed size $n = |V|$ \citep{bruno99sets}. Hereby, w.l.o.g., let $V = \{1,\dots, n\}$.

A set function $F:2^V \rightarrow \mathbb{R}$ is submodular if it exhibits a "diminishing returns" property \citep{krause14submodular}, namely if it satisfies:

\begin{equation}
  \label{eq:submod-condition}
  \forall S,T \subseteq V : S \subseteq T, i \notin T \mid F(S \cup i) - F(S) \geq F(T \cup i) - F(T)
\end{equation}

Intuitively this indicates that adding an element to a smaller set yields a larger effect, i.e. in terms of the set function $F$ value, than adding it to a larger one. This is a natural property in the context of summarization where adding more information to a large summary can be less effective than adding it to a smaller one. This property also makes them good candidates for modeling the concept of diversity \citep{tschiatschek16learning}.

\section{Supermodularity and Modularity}

Analogous to the use of submodular functions to model diversity, supermodular functions can be used to model complementarity or coherence between elements.

A set function is supermodular if it satisfies the condition in \eqref{eq:submod-condition} with the inequality sign reversed, i.e. a set function $F(S)$ is supermodular iff $-F(S)$ is submodular. Supermodularity is used extensively in economics to model complementarity between strategies in games \citep{amir2005supermodularity}.

Finally, if a function $F$ is both submodular and supermodular, i.e. it satisfies condition \eqref{eq:submod-condition} with equality, then it is said to be a modular function. These are the simplest examples of submodular or supermodular functions and are akin to linear functions but in a discrete domain \citep{krause14submodular}.

\section{Probabilistic Log-sub/supermodular Models}

This work is interested in a particular class of probability set functions, namely probabilistic log-submodular and log-supermodular models, which are probabilities $P(S)$ of the form \citep{djolonga14variational}:

\begin{equation}
  P(S) \propto \exp(F(S))
\end{equation}

Where $F(S)$ is a submodular or supermodular function, respectively. These models encompass many practical probabilistic models such as repulsive Ising models, Deterministic Point Processes (DPPs) and binary pairwise Markov Random Fields (MRFs) \citep{djolonga14variational, djolonga15scalable}.

This model class is of interest because the contributions of this work focus on extensions to a novel probabilistic log-submodular model proposed by \citet{tschiatschek16learning} which will be described in the next section.

\subsection{Partition Function}
\label{sec:general-z}

In log-submodular models the normalization constant $Z$ is known as the \textit{partition function} \citep{djolonga14variational}. $Z$ is necessary to fully determine the normalized model and compute quantities such as marginal probabilities, however its exact computation is known to be \#P-complete \citep{jerrum1990}. This makes it impossible in practice to perform such computations except for special cases of the model.

\section{Facility Location Diversity Model (FLID)}

Every modular function can be written as a sum of individual weights for each element $i \in V$ assuming a normalization such that $F(\emptyset) = 0$ \citep{krause14submodular}, i.e.

\begin{equation}
  F(S) = \sum_{i \in S} u(i)
\end{equation}

Where $u(i)$ is some function $u: V \rightarrow \mathbb{R}$. For convenience, denote $\mathbf{u}$ as the vector of modular weights where $u_{i} = u(i)$. Therefore any log-modular function can be written as:

\begin{equation}
  \label{eq:modular}
  P(S) \propto \exp{\left(\sum_{i \in S}u_{i}\right)}= \prod_{i \in S} \exp{\left(u_{i}\right)}
\end{equation}

This is the simplest log-submodular probability function and is the modular part for the FLID model proposed by \citet{tschiatschek16learning}. In this model, $\mathbf{u}$ can be thought of as modeling the relevance or utility of each element, for example in the context of spatial summarization this utility could be proportional to the popularity of a location or to how many times it has been photographed.

However, these utilities alone can not capture interactions between the elements in a set. To address this, \citet{tschiatschek16learning} propose an additional term that models set diversity, its objective is to characterize redundant elements and assign lower probabilities to sets that contain them.

This term is based on representing each element $i$ with an $L$-dimensional vector $\mathbf{w}^{b} \in \mathbb{R}^{L}_{\geq 0}$, where each dimension $1 \leq d \leq L$ captures a concept related to set diversity and $w^{b}_{i,d}$ quantifies the relevance of each element $i$ for each concept $d$ \citep{tschiatschek16learning}. Hereby, define $\mathbf{W}^{b} \in \mathbb{R}^{|V| \times L}_{\geq 0}$ as the matrix where the $i$-th row is the representation $\mathbf{w}^{b}$ of element $i$.

For each dimension $d$, the diversity of a set $S$ is quantified by $\max_{i \in S}{w^{b}_{i,d}} - \sum_{i \in S}{w^{b}_{i,d}}$. This assigns a negative value for sets that contain more than one element with nonzero weight $w^{b}_{\cdot,d}$ in that dimension \citep{tschiatschek16learning}. Equation \eqref{eq:diversity} shows the complete term, this sums over all $L$ dimensions to account the diversity in each concept.

\begin{equation}
  \label{eq:diversity}
  \mathrm{div}(S) = \sum_{d=1}^{L}\left(\max_{i \in S}{w^{b}_{i, d}} - \sum_{i \in S}{w^{b}_{i,d}}\right)
\end{equation}

Finally, the complete FLID model proposed by \citet{tschiatschek16learning} combines these two terms and is given by:

\begin{equation}
  \tag{FLID}
  P(S) = \frac{1}{Z}\exp{\left(\sum_{i \in S}u_{i} + \sum_{d=1}^{L}\left(\max_{i \in S}{w^{b}_{i, d}} - \sum_{i \in S}{w^{b}_{i,d}}\right)\right)}
  \label{eq:flid}
\end{equation}

\subsection{Partition Function}
\label{sec:flid-z}

As mentioned before, the computation of the partition function is generally intractable for log-submodular models. However, for FLID this can be computed in time $\mathcal{O}(|V|^{L+1})$ which is polynomial on the size of the ground set and significantly better than the cost of enumerating the powerset of $V$, i.e. $\mathcal{O}(2^{|V|})$ \citep{tschiatschek16learning}. Nevertheless, it is worth noting that this complexity is exponential in $L$ which means that the partition function computation is only pratical for a limited range of FLID models, namely those with $L \ll |V|$.

\subsection{Example: Two Landmarks}
\label{sec:flid-toy}

In order to better illustrate the model, consider a town with 3 popular locations: A town hall ($h$), a statue ($s$) and a fountain ($f$). Assume that historic data shows that visitors only take photos at either the town hall and the statue, or at the town hall and the fountain. This can be modeled with FLID by introducing a latent dimension representing some concept that is present in both the statue and the fountain but not in the town hall, e.g. "is not a building".

Concretely, let $V = \{h, s, f\}$ and $\mathbf{u} = \left(2, 2, 2\right)^{\intercal}$, indicating that all locations are equally popular. A suitable diversity weight matrix would then be $\mathbf{W}^{b} = \left(0, 20, 20\right)^{\intercal}$. Table \ref{tab:flid-toy-probs} shows the resulting probabilities of the subsets, accurately representing the aforementioned problem. Note that the probabilities are not exactly $\nicefrac{0.5}{0.5}$ for the sets of interest but this could be easily fixed by increasing the utilities to ensure that sets of size 2 have a larger unnormalized magnitude compared to individual ones.

\begin{table}
  \centering
  \caption{FLID probability distribution for the scenario in Example \ref{sec:flid-toy}.}
  \begin{tabular}{@{}ll@{}}
    \toprule
    $S$ & $P(S)$  \\
    \midrule
    $\{h,s\}, \{h,f\}$ & $\approx 0.41$ \\
    $\{h\}, \{s\}, \{f\}$ & $\approx 0.06$ \\
    $\{\}, \{s,f\}, \{h,s,f\}$ & $\approx 0.00$ \\
    \bottomrule
  \end{tabular}
  \label{tab:flid-toy-probs}
\end{table}

\section{Learning from Data}
\label{sec:learning}

Much of the interest in log-submodular models is concentrated on performing inference for known functions \citep{tschiatschek16learning}. However, \citet{tschiatschek16learning} explore how to learn such models from data, i.e. estimate the model parameters from observations with an unknown distribution.

Even though Maximum likelihood Estimation (MLE) is the commonly used method for the task of parameter estimation from data, it only works efficiently for normalized models \citep{Gutmann12NCE}. This makes it intractable for the general class of log-submodular models, and also a wide range of FLID models because the computation of $Z$ is exponential on $L$. \citet{Gutmann12NCE} propose an alternative method for parameter estimation in unnormalized models known as Noise Contrastive Estimation (NCE) and this is the method used by \citet{tschiatschek16learning} to learn the FLID model.

\subsection{NCE Learning}

The idea behind NCE is to transform the unsupervised learning task of estimating a probability density from data into a supervised classification task. In order to do this, the observed data $\mathcal{D}$, assumed to be drawn from an unknown distribution $P_{d}$, is compared to an artificially generated set of noise samples $\mathcal{N}$ drawn from a known distribution $P_{n}$ that can be efficiently normalized. The classification task is to maximize the likelihood of correctly discriminating each sample as either observed data or artificial noise.

Formally, denote $\mathcal{A}$ as the complete set of labeled samples, i.e. $\mathcal{A} = \{(S,Y_{s}) : S \in \mathcal{D} \cup \mathcal{N}\}$ where $Y_{s} = 1$ for $S \in \mathcal{D}$ and $Y_{s} = 0$ for $S \in \mathcal{N}$. Additionally, let $\nu$ be the noise-to-data ratio, i.e. $\nu = \nicefrac{|\mathcal{N}|}{|\mathcal{D}|}$.

The goal is to estimate the posterior probabilities $P(Y_{s} = 1 \mid S;\theta)$ and $P(Y_{s} = 0 \mid S;\theta)$, in order to discriminate noise from data samples. These probabilities are given by equations \eqref{eq:posterior-1} and \eqref{eq:posterior-2}.

\begin{align}
  P(Y_{s} &= 1 \mid S;\theta) = \frac{\hat{P}_{d}(S;\theta)}{\hat{P}_{d}(S;\theta) + \nu P_{n}(S)}
  \label{eq:posterior-1} \\
  P(Y_{s} &= 0 \mid S;\theta) = \frac{\nu P_{n}(S)}{\hat{P}_{d}(S;\theta) + \nu P_{n}(S)}
  \label{eq:posterior-2}
\end{align}

It is worth noting that $\hat{P}_{d}$ is used instead of $P_{d}$, because the real density is not known. As indicated by \citet{Gutmann12NCE}, $\hat{P}_{d}$ can be an unnormalized distribution for NCE where the partition function $Z$ is included in the set of parameters $\theta$ as $\hat{Z}$, hence resulting in an approximately normalized distribution after the optimization.

In order to obtain these posterior probabilities, the following conditional log-likelihood objective is maximized \citep{Gutmann12NCE}:

\begin{equation}
\label{eq:log-likelihood}
g(\theta) = \sum_{S \in \mathcal{D}}{\log{P(Y_{s} = 1 \mid S;\theta)}} + \sum_{S \in \mathcal{N}}{\log{P(Y_{s} = 0 \mid S;\theta)}}
\end{equation}

This maximization can be performed using a gradient-based optimization method such as Stochastic Gradient Descent (SGD).

\subsubsection{Practical Considerations}

A couple of considerations are necessary for obtaining good estimates with NCE \citep{Gutmann12NCE}:

\begin{enumerate}
  \item The parameterized probability function $\hat{P}_{d}(S;\theta)$ must be of the same family as the real distribution $P_{d}$, i.e. $\exists \theta^{*} \mid \hat{P}_{d}(S;\theta^{*}) = P_{d}$.
  \item The noise distribution $P_{n}$ is nonzero whenever $P_{d}$ is nonzero.
\end{enumerate}

Additionally, the following statements must be considered when choosing the noise distribution $P_{n}$ \citep{Gutmann12NCE}:

\begin{enumerate}
  \item A distribution that can be sampled easily.
  \item Noise that is as similar as possible to the data, otherwise the classification problem could be too easy.
  \item A noise sample as large as possible.
\end{enumerate}

\subsection{Learning FLID via NCE and SGD}

Stochastic Gradient Descent was used by \citet{tschiatschek16learning} to learn the FLID model through NCE. SGD is a gradient-based method that has proven effective in large-scale learning tasks due to its efficiency when the computation time is a limiting factor \citep{Bottou2010, Zhang2004}, hence making it appropriate for the scale of data sourced from the internet, such as public user photos in Flickr.

In each iteration, the gradient $\nabla \log{P(Y_{s} = y \mid S;\theta)}$ must be computed. For FLID, the parameter vector $\theta$ is composed by $\mathbf{u}, \mathbf{W}^{b}$ and $\hat{Z}$, and the corresponding gradients are given by:

\begin{align}
  \label{eq:gradient-flid-1}
  \nabla \log{P(Y_{s} = y \mid S;\theta)} &= \left(y - \frac{1}{1 + \nu \frac{P_{n}(S)}{\hat{P}_{d}(S;\theta)}}\right) \nabla \log{\hat{P}_{d}(S;\theta)} \\
  \label{eq:gradient-flid-2}
  \hat{P}_{d}(S;\theta) &= \frac{1}{\hat{Z}}P(S; \mathbf{u}, \mathbf{W}^{b}) \\
  \left(\nabla_{\mathbf{u}}\log{\hat{P}_{d}(S; \hat{Z}, \mathbf{u}, \mathbf{W}^{b})}\right)_{i} &= \begin{cases}
    1 &  \text{if}\ i \in S \\
    0 & \text{otherwise}
  \end{cases} \\
  \label{eq:gradient-flid-3}
  \left(\nabla_{\mathbf{W}^{b}}\log{\hat{P}_{d}(S; \hat{Z}, \mathbf{u}, \mathbf{W}^{b})}\right)_{i,d} &= \begin{cases}
    -1 & \text{if}\ i \in S\ \text{and}\ i \neq \argmax_{j \in S}{w^{b}_{j,d}} \\
    0 & \text{otherwise}
  \end{cases} \\
  \label{eq:gradient-flid-4}
  \nabla_{\hat{Z}}\log{\hat{P}_{d}(S; \hat{Z}, \mathbf{u}, \mathbf{W}^{b})}&= \frac{-1}{\hat{Z}}
\end{align}

Where $P(S;\mathbf{u}, \mathbf{W}^{b})$ is the unnormalized \eqref{eq:flid} equation, $\left(\nabla_{\mathbf{u}}\cdot \right)_{i}$ represents the $i$-th entry of the gradient with respect to $\mathbf{u}$ and $\left(\nabla_{\mathbf{W}^{b}}\cdot\right)_{i,d}$ represents the $(i,d)$-th entry of the sub-gradient with respect to $\mathbf{W}^{b}$ \citep{tschiatschek16learning}.

\subsubsection{Computational Complexity}

As described by \citet{tschiatschek16learning}, the running time requirement for calculating the sub-gradient for FLID is $\mathcal{O}(L|S|)$, and for a complete pass over data and noise samples is then $\mathcal{O}(|\mathcal{D}\cup\mathcal{N}|\kappa L)$ where $\kappa = \max_{S \in \mathcal{D}\cup\mathcal{N}}{|S|}$. This shows that learning is efficient as it is only linear on the size of the data and the noise, assuming small values for $\kappa$ and $L$.

\subsection{AdaGrad}
\label{sec:adagrad}

A commonly used learning rate function for SGD is $\eta(t) = \eta_{0}t^{-p}$, which offers the best convergence speed in theory and also does it often in practice \citep{bottou2012stochastic}, however it can lead to poor performance due to slow rates of convergence to the solution \citep{darken1992towards}. By contrast, choosing a larger $\eta_{0}$ may not lead to better results due to the instability in the parameters for small $t$ \citep{Darken1990}. 

This behavior was observed during the experiments with real data to be presented in later sections, therefore an alternative for the learning rate was sought. In particular, Adaptive Gradient (AdaGrad) was implemented.

AdaGrad was proposed by \citet{Duchi2011adagrad}, the idea of this method is to adapt the learning rate for each parameter in $\theta$ individually in a way that frequently updated parameters have slower learning rates while infrequent parameters have larger ones. The intuition is that an update to a rare parameter is more informative than one to a parameter that is frequently updated.

Concretely, with AdaGrad the update step for each parameter in $\theta$ is given by:

\begin{equation}
\theta^{\tau + 1}_{j} \leftarrow \theta^{\tau}_{j} - \frac{\eta}{\sqrt{G_{j,j}}} \nabla g(\theta)^{\tau}_{j}
\end{equation}

Where $G_{j,j}$ contains the information about past gradients for parameter $j$ and is given by:

\begin{equation}
G_{j,j} = \sum_{t=1}^{\tau} \left(\nabla g(\theta)^{t}_{j}\right)^{2}
\label{eq:adagrad-g}
\end{equation}

In equation \eqref{eq:adagrad-g} $\nabla g(\theta)^{t}_{j}$ denotes the $j$-th entry of the gradient at time step $t$.

It is worth noting that AdaGrad does not incur in an significant increase in running time or memory requirements for the training which makes it inexpensive to include in the implementation of NCE.
